{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01 : Standard ConvNets - demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of original LeNet5 Convolutional Neural Networks:<br>\n",
    "Gradient-based learning applied to document recognition<br>\n",
    "Y LeCun, L Bottou, Y Bengio, P Haffner<br>\n",
    "Proceedings of the IEEE 86 (11), 2278-2324<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pdb #pdb.set_trace()\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    torch.cuda.manual_seed(1)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from utils import check_mnist_dataset_exists\n",
    "data_path=check_mnist_dataset_exists()\n",
    "\n",
    "train_data=torch.load(data_path+'mnist/train_data.pt')\n",
    "train_label=torch.load(data_path+'mnist/train_label.pt')\n",
    "test_data=torch.load(data_path+'mnist/test_data.pt')\n",
    "test_label=torch.load(data_path+'mnist/test_label.pt')\n",
    "\n",
    "print(train_data.size())\n",
    "print(test_data.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet LeNet5\n",
    "### Layers: CL32-MP4-CL64-MP4-FC512-FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class definition\n",
    "class ConvNet_LeNet5(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        \n",
    "        print('ConvNet: LeNet5\\n')\n",
    "        \n",
    "        super(ConvNet_LeNet5, self).__init__()\n",
    "        \n",
    "        Nx, Ny, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F = net_parameters\n",
    "        FC1Fin = CL2_F*(Nx//4)**2\n",
    "        \n",
    "        # graph CL1\n",
    "        self.conv1 = nn.Conv2d(1, CL1_F, CL1_K, padding=(2, 2))\n",
    "        Fin = CL1_K**2; Fout = CL1_F;\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        self.conv1.weight.data.uniform_(-scale, scale)\n",
    "        self.conv1.bias.data.fill_(0.0)\n",
    "        \n",
    "        # graph CL2\n",
    "        self.conv2 = nn.Conv2d(CL1_F, CL2_F, CL2_K, padding=(2, 2))\n",
    "        Fin = CL1_F*CL2_K**2; Fout = CL2_F;\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        self.conv2.weight.data.uniform_(-scale, scale)\n",
    "        self.conv2.bias.data.fill_(0.0)\n",
    "        \n",
    "        # FC1\n",
    "        self.fc1 = nn.Linear(FC1Fin, FC1_F) \n",
    "        Fin = FC1Fin; Fout = FC1_F;\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        self.fc1.weight.data.uniform_(-scale, scale)\n",
    "        self.fc1.bias.data.fill_(0.0)\n",
    "        self.FC1Fin = FC1Fin\n",
    "        \n",
    "        # FC2\n",
    "        self.fc2 = nn.Linear(FC1_F, FC2_F)\n",
    "        Fin = FC1_F; Fout = FC2_F;\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        self.fc2.weight.data.uniform_(-scale, scale)\n",
    "        self.fc2.bias.data.fill_(0.0)\n",
    "        \n",
    "        # max pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "         \n",
    "        \n",
    "    def forward(self, x, d):\n",
    "        \n",
    "        # CL1\n",
    "        x = self.conv1(x)    \n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # CL2\n",
    "        x = self.conv2(x)    \n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # FC1\n",
    "        x = x.permute(0,3,2,1).contiguous() # reshape from pytorch array to tensorflow array\n",
    "        x = x.view(-1, self.FC1Fin)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x  = nn.Dropout(d)(x)\n",
    "        \n",
    "        # FC2\n",
    "        x = self.fc2(x)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def loss(self, y, y_target, l2_regularization):\n",
    "    \n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "\n",
    "        l2_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            data = param* param\n",
    "            l2_loss += data.sum()\n",
    "           \n",
    "        loss += 0.5* l2_regularization* l2_loss\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr, momentum=0.9 )\n",
    "        \n",
    "        return update\n",
    "        \n",
    "           \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "    def evaluation(self, y_predicted, test_l):\n",
    "    \n",
    "        _, class_predicted = torch.max(y_predicted.data, 1)\n",
    "        return 100.0* (class_predicted == test_l).sum().float()/ y_predicted.size(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing network to delete\n",
      "\n",
      "ConvNet: LeNet5\n",
      "\n",
      "ConvNet_LeNet5(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "num_epochs= 20 , train_size= 60000 , nb_iter= 12000\n",
      "epoch= 1, i=  100, loss(batch)= 0.2990, accuray(batch)= 95.00\n",
      "epoch= 1, i=  200, loss(batch)= 0.1742, accuray(batch)= 97.00\n",
      "epoch= 1, i=  300, loss(batch)= 0.1795, accuray(batch)= 98.00\n",
      "epoch= 1, i=  400, loss(batch)= 0.1604, accuray(batch)= 99.00\n",
      "epoch= 1, i=  500, loss(batch)= 0.1665, accuray(batch)= 99.00\n",
      "epoch= 1, i=  600, loss(batch)= 0.1315, accuray(batch)= 98.00\n",
      "epoch= 1, loss(train)= 0.290, accuracy(train)= 94.107, time= 3.693, lr= 0.05000\n",
      "  accuracy(test) = 98.690 %, time= 0.260\n",
      "epoch= 2, i=  100, loss(batch)= 0.2093, accuray(batch)= 99.00\n",
      "epoch= 2, i=  200, loss(batch)= 0.1062, accuray(batch)= 100.00\n",
      "epoch= 2, i=  300, loss(batch)= 0.1711, accuray(batch)= 97.00\n",
      "epoch= 2, i=  400, loss(batch)= 0.1047, accuray(batch)= 100.00\n",
      "epoch= 2, i=  500, loss(batch)= 0.1064, accuray(batch)= 100.00\n",
      "epoch= 2, i=  600, loss(batch)= 0.0957, accuray(batch)= 100.00\n",
      "epoch= 2, loss(train)= 0.148, accuracy(train)= 98.433, time= 3.255, lr= 0.04750\n",
      "  accuracy(test) = 99.030 %, time= 0.173\n",
      "epoch= 3, i=  100, loss(batch)= 0.1900, accuray(batch)= 99.00\n",
      "epoch= 3, i=  200, loss(batch)= 0.1035, accuray(batch)= 100.00\n",
      "epoch= 3, i=  300, loss(batch)= 0.1001, accuray(batch)= 100.00\n",
      "epoch= 3, i=  400, loss(batch)= 0.0907, accuray(batch)= 100.00\n",
      "epoch= 3, i=  500, loss(batch)= 0.0984, accuray(batch)= 99.00\n",
      "epoch= 3, i=  600, loss(batch)= 0.0932, accuray(batch)= 100.00\n",
      "epoch= 3, loss(train)= 0.125, accuracy(train)= 98.762, time= 3.169, lr= 0.04512\n",
      "  accuracy(test) = 99.080 %, time= 0.170\n",
      "epoch= 4, i=  100, loss(batch)= 0.0913, accuray(batch)= 99.00\n",
      "epoch= 4, i=  200, loss(batch)= 0.0804, accuray(batch)= 100.00\n",
      "epoch= 4, i=  300, loss(batch)= 0.1109, accuray(batch)= 98.00\n",
      "epoch= 4, i=  400, loss(batch)= 0.0937, accuray(batch)= 100.00\n",
      "epoch= 4, i=  500, loss(batch)= 0.0766, accuray(batch)= 100.00\n",
      "epoch= 4, i=  600, loss(batch)= 0.1682, accuray(batch)= 97.00\n",
      "epoch= 4, loss(train)= 0.107, accuracy(train)= 99.027, time= 3.204, lr= 0.04287\n",
      "  accuracy(test) = 99.060 %, time= 0.172\n",
      "epoch= 5, i=  100, loss(batch)= 0.0707, accuray(batch)= 100.00\n",
      "epoch= 5, i=  200, loss(batch)= 0.1156, accuray(batch)= 99.00\n",
      "epoch= 5, i=  300, loss(batch)= 0.0775, accuray(batch)= 100.00\n",
      "epoch= 5, i=  400, loss(batch)= 0.0734, accuray(batch)= 100.00\n",
      "epoch= 5, i=  500, loss(batch)= 0.0703, accuray(batch)= 100.00\n",
      "epoch= 5, i=  600, loss(batch)= 0.0883, accuray(batch)= 99.00\n",
      "epoch= 5, loss(train)= 0.094, accuracy(train)= 99.213, time= 3.163, lr= 0.04073\n",
      "  accuracy(test) = 99.230 %, time= 0.145\n",
      "epoch= 6, i=  100, loss(batch)= 0.0696, accuray(batch)= 100.00\n",
      "epoch= 6, i=  200, loss(batch)= 0.0791, accuray(batch)= 100.00\n",
      "epoch= 6, i=  300, loss(batch)= 0.0732, accuray(batch)= 100.00\n",
      "epoch= 6, i=  400, loss(batch)= 0.0737, accuray(batch)= 99.00\n",
      "epoch= 6, i=  500, loss(batch)= 0.0733, accuray(batch)= 99.00\n",
      "epoch= 6, i=  600, loss(batch)= 0.0726, accuray(batch)= 100.00\n",
      "epoch= 6, loss(train)= 0.086, accuracy(train)= 99.217, time= 2.819, lr= 0.03869\n",
      "  accuracy(test) = 99.310 %, time= 0.153\n",
      "epoch= 7, i=  100, loss(batch)= 0.0715, accuray(batch)= 100.00\n",
      "epoch= 7, i=  200, loss(batch)= 0.0641, accuray(batch)= 100.00\n",
      "epoch= 7, i=  300, loss(batch)= 0.0746, accuray(batch)= 100.00\n",
      "epoch= 7, i=  400, loss(batch)= 0.1187, accuray(batch)= 98.00\n",
      "epoch= 7, i=  500, loss(batch)= 0.0747, accuray(batch)= 99.00\n",
      "epoch= 7, i=  600, loss(batch)= 0.0645, accuray(batch)= 100.00\n",
      "epoch= 7, loss(train)= 0.079, accuracy(train)= 99.327, time= 2.818, lr= 0.03675\n",
      "  accuracy(test) = 99.330 %, time= 0.145\n",
      "epoch= 8, i=  100, loss(batch)= 0.0586, accuray(batch)= 100.00\n",
      "epoch= 8, i=  200, loss(batch)= 0.0685, accuray(batch)= 99.00\n",
      "epoch= 8, i=  300, loss(batch)= 0.0621, accuray(batch)= 100.00\n",
      "epoch= 8, i=  400, loss(batch)= 0.0746, accuray(batch)= 99.00\n",
      "epoch= 8, i=  500, loss(batch)= 0.0975, accuray(batch)= 99.00\n",
      "epoch= 8, i=  600, loss(batch)= 0.1328, accuray(batch)= 99.00\n",
      "epoch= 8, loss(train)= 0.073, accuracy(train)= 99.393, time= 2.815, lr= 0.03492\n",
      "  accuracy(test) = 99.300 %, time= 0.146\n",
      "epoch= 9, i=  100, loss(batch)= 0.0553, accuray(batch)= 100.00\n",
      "epoch= 9, i=  200, loss(batch)= 0.0548, accuray(batch)= 100.00\n",
      "epoch= 9, i=  300, loss(batch)= 0.0669, accuray(batch)= 99.00\n",
      "epoch= 9, i=  400, loss(batch)= 0.0599, accuray(batch)= 100.00\n",
      "epoch= 9, i=  500, loss(batch)= 0.0804, accuray(batch)= 98.00\n",
      "epoch= 9, i=  600, loss(batch)= 0.0546, accuray(batch)= 100.00\n",
      "epoch= 9, loss(train)= 0.068, accuracy(train)= 99.445, time= 2.832, lr= 0.03317\n",
      "  accuracy(test) = 99.310 %, time= 0.146\n",
      "epoch= 10, i=  100, loss(batch)= 0.0558, accuray(batch)= 100.00\n",
      "epoch= 10, i=  200, loss(batch)= 0.0578, accuray(batch)= 100.00\n",
      "epoch= 10, i=  300, loss(batch)= 0.0600, accuray(batch)= 100.00\n",
      "epoch= 10, i=  400, loss(batch)= 0.0518, accuray(batch)= 100.00\n",
      "epoch= 10, i=  500, loss(batch)= 0.0719, accuray(batch)= 99.00\n",
      "epoch= 10, i=  600, loss(batch)= 0.0699, accuray(batch)= 100.00\n",
      "epoch= 10, loss(train)= 0.064, accuracy(train)= 99.510, time= 3.123, lr= 0.03151\n",
      "  accuracy(test) = 99.310 %, time= 0.182\n",
      "epoch= 11, i=  100, loss(batch)= 0.0467, accuray(batch)= 100.00\n",
      "epoch= 11, i=  200, loss(batch)= 0.0850, accuray(batch)= 99.00\n",
      "epoch= 11, i=  300, loss(batch)= 0.0475, accuray(batch)= 100.00\n",
      "epoch= 11, i=  400, loss(batch)= 0.0577, accuray(batch)= 100.00\n",
      "epoch= 11, i=  500, loss(batch)= 0.0503, accuray(batch)= 100.00\n",
      "epoch= 11, i=  600, loss(batch)= 0.0511, accuray(batch)= 100.00\n",
      "epoch= 11, loss(train)= 0.059, accuracy(train)= 99.547, time= 2.969, lr= 0.02994\n",
      "  accuracy(test) = 99.200 %, time= 0.153\n",
      "epoch= 12, i=  100, loss(batch)= 0.0510, accuray(batch)= 100.00\n",
      "epoch= 12, i=  200, loss(batch)= 0.0562, accuray(batch)= 100.00\n",
      "epoch= 12, i=  300, loss(batch)= 0.0627, accuray(batch)= 99.00\n",
      "epoch= 12, i=  400, loss(batch)= 0.0536, accuray(batch)= 99.00\n",
      "epoch= 12, i=  500, loss(batch)= 0.0555, accuray(batch)= 99.00\n",
      "epoch= 12, i=  600, loss(batch)= 0.0465, accuray(batch)= 100.00\n",
      "epoch= 12, loss(train)= 0.058, accuracy(train)= 99.513, time= 2.836, lr= 0.02844\n",
      "  accuracy(test) = 99.170 %, time= 0.146\n",
      "epoch= 13, i=  100, loss(batch)= 0.0760, accuray(batch)= 99.00\n",
      "epoch= 13, i=  200, loss(batch)= 0.0609, accuray(batch)= 99.00\n",
      "epoch= 13, i=  300, loss(batch)= 0.0714, accuray(batch)= 99.00\n",
      "epoch= 13, i=  400, loss(batch)= 0.0535, accuray(batch)= 99.00\n",
      "epoch= 13, i=  500, loss(batch)= 0.0432, accuray(batch)= 100.00\n",
      "epoch= 13, i=  600, loss(batch)= 0.0543, accuray(batch)= 100.00\n",
      "epoch= 13, loss(train)= 0.056, accuracy(train)= 99.555, time= 2.834, lr= 0.02702\n",
      "  accuracy(test) = 99.340 %, time= 0.146\n",
      "epoch= 14, i=  100, loss(batch)= 0.0477, accuray(batch)= 100.00\n",
      "epoch= 14, i=  200, loss(batch)= 0.0575, accuray(batch)= 99.00\n",
      "epoch= 14, i=  300, loss(batch)= 0.0418, accuray(batch)= 100.00\n",
      "epoch= 14, i=  400, loss(batch)= 0.0548, accuray(batch)= 99.00\n",
      "epoch= 14, i=  500, loss(batch)= 0.0413, accuray(batch)= 100.00\n",
      "epoch= 14, i=  600, loss(batch)= 0.0443, accuray(batch)= 100.00\n",
      "epoch= 14, loss(train)= 0.053, accuracy(train)= 99.613, time= 3.123, lr= 0.02567\n",
      "  accuracy(test) = 99.340 %, time= 0.145\n",
      "epoch= 15, i=  100, loss(batch)= 0.0510, accuray(batch)= 100.00\n",
      "epoch= 15, i=  200, loss(batch)= 0.0495, accuray(batch)= 100.00\n",
      "epoch= 15, i=  300, loss(batch)= 0.0517, accuray(batch)= 100.00\n",
      "epoch= 15, i=  400, loss(batch)= 0.0416, accuray(batch)= 100.00\n",
      "epoch= 15, i=  500, loss(batch)= 0.0465, accuray(batch)= 100.00\n",
      "epoch= 15, i=  600, loss(batch)= 0.0628, accuray(batch)= 99.00\n",
      "epoch= 15, loss(train)= 0.051, accuracy(train)= 99.623, time= 3.005, lr= 0.02438\n",
      "  accuracy(test) = 99.370 %, time= 0.166\n",
      "epoch= 16, i=  100, loss(batch)= 0.0430, accuray(batch)= 100.00\n",
      "epoch= 16, i=  200, loss(batch)= 0.0849, accuray(batch)= 98.00\n",
      "epoch= 16, i=  300, loss(batch)= 0.0549, accuray(batch)= 99.00\n",
      "epoch= 16, i=  400, loss(batch)= 0.0517, accuray(batch)= 99.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 16, i=  500, loss(batch)= 0.0708, accuray(batch)= 99.00\n",
      "epoch= 16, i=  600, loss(batch)= 0.0436, accuray(batch)= 100.00\n",
      "epoch= 16, loss(train)= 0.050, accuracy(train)= 99.663, time= 3.055, lr= 0.02316\n",
      "  accuracy(test) = 99.410 %, time= 0.183\n",
      "epoch= 17, i=  100, loss(batch)= 0.0486, accuray(batch)= 100.00\n",
      "epoch= 17, i=  200, loss(batch)= 0.0461, accuray(batch)= 99.00\n",
      "epoch= 17, i=  300, loss(batch)= 0.0400, accuray(batch)= 100.00\n",
      "epoch= 17, i=  400, loss(batch)= 0.0391, accuray(batch)= 100.00\n",
      "epoch= 17, i=  500, loss(batch)= 0.0408, accuray(batch)= 100.00\n",
      "epoch= 17, i=  600, loss(batch)= 0.0434, accuray(batch)= 100.00\n",
      "epoch= 17, loss(train)= 0.048, accuracy(train)= 99.713, time= 3.201, lr= 0.02201\n",
      "  accuracy(test) = 99.390 %, time= 0.156\n",
      "epoch= 18, i=  100, loss(batch)= 0.0740, accuray(batch)= 98.00\n",
      "epoch= 18, i=  200, loss(batch)= 0.0428, accuray(batch)= 100.00\n",
      "epoch= 18, i=  300, loss(batch)= 0.0474, accuray(batch)= 99.00\n",
      "epoch= 18, i=  400, loss(batch)= 0.0389, accuray(batch)= 100.00\n",
      "epoch= 18, i=  500, loss(batch)= 0.0539, accuray(batch)= 99.00\n",
      "epoch= 18, i=  600, loss(batch)= 0.0444, accuray(batch)= 100.00\n",
      "epoch= 18, loss(train)= 0.047, accuracy(train)= 99.665, time= 3.126, lr= 0.02091\n",
      "  accuracy(test) = 99.330 %, time= 0.153\n",
      "epoch= 19, i=  100, loss(batch)= 0.0363, accuray(batch)= 100.00\n",
      "epoch= 19, i=  200, loss(batch)= 0.0380, accuray(batch)= 100.00\n",
      "epoch= 19, i=  300, loss(batch)= 0.0362, accuray(batch)= 100.00\n",
      "epoch= 19, i=  400, loss(batch)= 0.0636, accuray(batch)= 99.00\n",
      "epoch= 19, i=  500, loss(batch)= 0.0360, accuray(batch)= 100.00\n",
      "epoch= 19, i=  600, loss(batch)= 0.0458, accuray(batch)= 100.00\n",
      "epoch= 19, loss(train)= 0.046, accuracy(train)= 99.688, time= 3.070, lr= 0.01986\n",
      "  accuracy(test) = 99.370 %, time= 0.146\n",
      "epoch= 20, i=  100, loss(batch)= 0.0401, accuray(batch)= 100.00\n",
      "epoch= 20, i=  200, loss(batch)= 0.0414, accuray(batch)= 100.00\n",
      "epoch= 20, i=  300, loss(batch)= 0.0531, accuray(batch)= 100.00\n",
      "epoch= 20, i=  400, loss(batch)= 0.0394, accuray(batch)= 100.00\n",
      "epoch= 20, i=  500, loss(batch)= 0.0441, accuray(batch)= 99.00\n",
      "epoch= 20, i=  600, loss(batch)= 0.0412, accuray(batch)= 100.00\n",
      "epoch= 20, loss(train)= 0.044, accuracy(train)= 99.730, time= 3.098, lr= 0.01887\n",
      "  accuracy(test) = 99.300 %, time= 0.156\n"
     ]
    }
   ],
   "source": [
    "# Delete existing network if exists\n",
    "try:\n",
    "    del net\n",
    "    print('Delete existing network\\n')\n",
    "except NameError:\n",
    "    print('No existing network to delete\\n')\n",
    "\n",
    "\n",
    "\n",
    "# network parameters\n",
    "Nx = Ny = 28\n",
    "CL1_F = 32\n",
    "CL1_K = 5\n",
    "CL2_F = 64\n",
    "CL2_K = 5\n",
    "FC1_F = 512\n",
    "FC2_F = 10\n",
    "net_parameters = [Nx, Ny, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
    "\n",
    "\n",
    "# instantiate the object net of the class \n",
    "net = ConvNet_LeNet5(net_parameters)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "print(net)\n",
    "\n",
    "\n",
    "# Weights\n",
    "L = list(net.parameters())\n",
    "\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.05\n",
    "dropout_value = 0.5\n",
    "l2_regularization = 5e-4 \n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "train_size = train_data.shape[0]\n",
    "nb_iter = int(num_epochs * train_size) // batch_size\n",
    "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "global_lr = learning_rate\n",
    "global_step = 0\n",
    "decay = 0.95\n",
    "decay_steps = train_size\n",
    "lr = learning_rate\n",
    "optimizer = net.update(lr) \n",
    "\n",
    "\n",
    "# loop over epochs\n",
    "indices = collections.deque()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # reshuffle \n",
    "    indices.extend(np.random.permutation(train_size)) # rand permutation\n",
    "    \n",
    "    # reset time\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # extract batches\n",
    "    running_loss = 0.0\n",
    "    running_accuray = 0\n",
    "    running_total = 0\n",
    "    while len(indices) >= batch_size:\n",
    "        \n",
    "        # extract batches\n",
    "        batch_idx = [indices.popleft() for i in range(batch_size)]\n",
    "        train_x, train_y = train_data[batch_idx,:].unsqueeze(dim=1), train_label[batch_idx]\n",
    "        train_x = Variable( torch.FloatTensor(train_x).type(dtypeFloat) , requires_grad=False)\n",
    "        train_y = Variable( torch.LongTensor(train_y).type(dtypeLong) , requires_grad=False) \n",
    "            \n",
    "        # Forward \n",
    "        y = net.forward(train_x, dropout_value)\n",
    "        loss = net.loss(y,train_y,l2_regularization) \n",
    "        loss_train = loss.data.item()\n",
    "        \n",
    "        # Accuracy\n",
    "        acc_train = net.evaluation(y,train_y.data)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update \n",
    "        global_step += batch_size # to update learning rate\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss, accuracy\n",
    "        running_loss += loss_train\n",
    "        running_accuray += acc_train\n",
    "        running_total += 1\n",
    "        \n",
    "        # print        \n",
    "        if not running_total%100: # print every x mini-batches\n",
    "            print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch+1, running_total, loss_train, acc_train))\n",
    "          \n",
    "       \n",
    "    # print \n",
    "    t_stop = time.time() - t_start\n",
    "    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' % \n",
    "          (epoch+1, running_loss/running_total, running_accuray/running_total, t_stop, lr))\n",
    " \n",
    "\n",
    "    # update learning rate \n",
    "    lr = global_lr * pow( decay , float(global_step// decay_steps) )\n",
    "    optimizer = net.update_learning_rate(optimizer, lr)\n",
    "    \n",
    "    \n",
    "    # Test set\n",
    "    running_accuray_test = 0\n",
    "    running_total_test = 0\n",
    "    indices_test = collections.deque()\n",
    "    indices_test.extend(range(test_data.shape[0]))\n",
    "    t_start_test = time.time()\n",
    "    while len(indices_test) >= batch_size:\n",
    "        batch_idx_test = [indices_test.popleft() for i in range(batch_size)]\n",
    "        test_x, test_y = test_data[batch_idx_test,:].unsqueeze(dim=1), test_label[batch_idx_test]\n",
    "        test_x = Variable( torch.FloatTensor(test_x).type(dtypeFloat) , requires_grad=False)\n",
    "        test_y = Variable( torch.LongTensor(test_y).type(dtypeLong) , requires_grad=False) \n",
    "        y = net.forward(test_x, 0.0)        \n",
    "        acc_test = net.evaluation(y,test_y.data)\n",
    "        running_accuray_test += acc_test\n",
    "        running_total_test += 1\n",
    "    t_stop_test = time.time() - t_start_test\n",
    "    print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))  \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
