{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01 : Deep Q-Learning (DQN) - demo\n",
    "\n",
    "Author: Adam Paszke<br>\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "Cart pole dataset:<br>\n",
    "https://github.com/openai/gym/wiki/CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # OpenAI library\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load the environment CartPole from OpenAI\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# if gpu available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A memory that store training data (s, a, s', r)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Class of experience replay\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    #  Store the observed transitions (s, a, s', r)\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    #  Select a random batch of transitions for training \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class of Deep Q-Networks (DQN)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2) # output has 2 dimensions = Left or Right move\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract images=states=s generated by the environment. \n",
    "\n",
    "screen_width = 600 # from gym\n",
    "def get_cart_location():\n",
    "    \n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # middle of cart\n",
    "\n",
    "# Images will be transformed by torchvision from [3, 160, 320] to [3, 40, 80].\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    \n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1)) # transpose into torch order (CHW)\n",
    "    screen = screen[:, 160:320] # Strip off the top and bottom of the screen\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    screen = screen[:, :, slice_range] # Strip off the edges so that we have\n",
    "                                       # a square image centered on a cart.\n",
    "    # Convert to float, re-scale, convert to torch tensor\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    screen = resize(screen).unsqueeze(0).to(device) # Resize, and add a batch dimension (BCHW)\n",
    "    return screen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Instantiate DQN networks\n",
    "policy_net = DQN().to(device) # policy_network\n",
    "target_net = DQN().to(device) # policy_network_frozen : it is used to deal with non-stationarity\n",
    "target_net.load_state_dict(policy_net.state_dict()) # policy_network_frozen = policy network\n",
    "target_net.eval() # policy_network_frozen not updated (no backpropagation)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "\n",
    "# Initialize the replay memory\n",
    "memory = ReplayMemory(10000) \n",
    "\n",
    "# Function that samples an action (left or right move) from policy network\n",
    "# The sampling function that use either\n",
    "# 1. A random sampling of the action\n",
    "# 2. A Bernoulli sampling from the policy network\n",
    "# The chance of random sampling decreases over time\n",
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    \n",
    "    global steps_done\n",
    "    sample = random.random() # generates a random float uniformly in [0.0, 1.0)\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1) # sample one action from the policy network\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long) # random sampling of one action\n",
    "\n",
    "# Plot the average of the episode duration over 100 episodes.\n",
    "# Note: The plotting (orange) starts from the 100th episode.\n",
    "episode_durations = []\n",
    "def plot_durations():\n",
    "    \n",
    "    plt.figure(2)\n",
    "    #plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them \n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    #plt.pause(1) # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        #display.display(plt.gcf())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def optimize_model():\n",
    "    \n",
    "    # nothing to train if memory is empty or less than the batch size\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # sample a batch of transitions (s, a, s', r) from the replay memory \n",
    "    transitions = memory.sample(BATCH_SIZE) # \n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # compute next states=s' that are not final states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    # concatenate next states=s'\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    # concatenate states=s, actions=a, and reward=r\n",
    "    state_batch = torch.cat(batch.state) # s\n",
    "    action_batch = torch.cat(batch.action) # a\n",
    "    reward_batch = torch.cat(batch.reward) # r\n",
    "\n",
    "    # compute Q_w(a|s)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch) \n",
    "\n",
    "    # compute max_a' Q_w#(a'|s'), w# = policy_network_frozen\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach() \n",
    "    \n",
    "    # compute expected Q value : r + gamma . max_a' Q_w#(a'|s')\n",
    "    expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    # Compute Huber loss : \n",
    "    # sum_batch=(s,a,s',r) huber_loss( Q_w(a|s) - (r+g.max_a' Q_w#(a'|s')) )\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # backpropagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # clip the gradient value to prevent the exploding gradient problem\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the episodes\n",
    "num_episodes = 10000\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    \n",
    "    # Roll-out the episode\n",
    "    for t in count():\n",
    "        \n",
    "        # select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "            \n",
    "    # update the target network = policy_network_frozen with the current policy_network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close() # close the render window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # close the render window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
