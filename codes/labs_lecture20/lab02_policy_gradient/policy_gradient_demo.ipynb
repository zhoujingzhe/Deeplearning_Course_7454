{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02 : Gradient Policy Network - demo\n",
    "\n",
    "Author: FAIR<br>\n",
    "https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py\n",
    "\n",
    "Cart pole dataset:<br>\n",
    "https://github.com/openai/gym/wiki/CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Load the environment CartPole from OpenAI\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# class dictionary\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class of policy network\n",
    "class Policy(nn.Module): # OK\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function that samples an action (left or right move) from policy network\n",
    "def select_action(state): \n",
    "    \n",
    "    # state=s\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    \n",
    "    # probability of action a in state s\n",
    "    probs = policy(state)\n",
    "    \n",
    "    # sample action a with Bernoulli sampling\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    # compute and store log probability of selected action\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    \n",
    "    return action.item()\n",
    "\n",
    "\n",
    "# Function that compute the expected discounted reward when episode is done\n",
    "# and backpropagate to update the policy network\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "def finish_episode():\n",
    "    \n",
    "    # initialize\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    \n",
    "    # compute the return at each time step (with backward for loop)\n",
    "    for r in policy.rewards[::-1]: \n",
    "        R = r + args.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    # center the rewards, and make the variance of rewards equal to 1\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "        \n",
    "    # backpropagate\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # delete rewards and log_probs to prepare for next episode\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.99, 'seed': 1, 'render': True, 'log_interval': 10}\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "args = DotDict()\n",
    "args.gamma = 0.99\n",
    "args.seed = 1\n",
    "args.render = True\n",
    "args.log_interval = 10\n",
    "print(args)\n",
    "\n",
    "\n",
    "# initialize the environment with the same seed/initialization value\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "# Instantiate the policy network\n",
    "policy = Policy()\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    running_reward = 10\n",
    "    for i_episode in count(1): # consider multiple episodes\n",
    "        \n",
    "        state = env.reset() # reset the environment\n",
    "        \n",
    "        # draw an episode until it finishes or stop a t=10000. \n",
    "        for t in range(10000):  \n",
    "            action = select_action(state) # select action=a from state=s\n",
    "            state, reward, done, _ = env.step(action) # receive next state=s' and reward=r\n",
    "            if args.render:\n",
    "                env.render() # see the state\n",
    "            policy.rewards.append(reward) # store immediate rewards \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = running_reward * 0.99 + t * 0.01 # current reward = 99% of running reward\n",
    "                                                          #  + 1% last episode length \n",
    "        \n",
    "        finish_episode() # when episode is done, compute the expected discounted reward,\n",
    "                         # and backpropagate to update the policy network\n",
    "        \n",
    "        # display results every log_interval=10 episodes\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "                i_episode, t, running_reward))\n",
    "        \n",
    "        # stop the training when the reward is high enough\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "            \n",
    "main()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # close the render window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
